{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value for Care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Value of Care\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.load(\"/Users/seankeefe/Desktop/Payment_and_Value_of_Care_Hospital.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "#ReqData = \"/Users/seankeefe/Desktop/Hospital_Revised_Flatfiles/Payment_and_Value_of_Care-Hospital.csv\"\n",
    "#df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(ReqData)\n",
    "\n",
    "df.registerTempTable('Data')\n",
    "\n",
    "df2 = spark.read.load(\"/Users/seankeefe/Desktop/Payment-National.csv\",format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "#NationalRate = \"/Users/seankeefe/Desktop/Hospital_Revised_Flatfiles/Payment-National.csv\"\n",
    "#df2 = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(NationalRate)\n",
    "\n",
    "df2.registerTempTable('Master')\n",
    "\n",
    "\n",
    "category= \"SELECT Measure_Name from Master\"\n",
    "categoryName = spark.sql(category)\n",
    "\n",
    "\n",
    "state= \"SELECT DISTINCT State from Data order by State\"\n",
    "stateName = spark.sql(state)\n",
    "\n",
    "_dataList = categoryName.collect()\n",
    "_dataListState = stateName.collect()\n",
    "# print(_dataList)\n",
    "_category =[]\n",
    "_selectedCat = [\"NA\"]\n",
    "_category.append(\"NA\")\n",
    "for i in range(len(_dataList)):\n",
    "    _category.append(_dataList[i][0])\n",
    "\n",
    "    \n",
    "_state =[]\n",
    "_selectedSta = [\"All\"]\n",
    "_state.append(\"All\")\n",
    "for i in range(len(_dataListState)):\n",
    "    _state.append(_dataListState[i][0])    \n",
    "    \n",
    "selectState = widgets.Dropdown(\n",
    "    options=_state,\n",
    "    description='State:',\n",
    "    disabled=False\n",
    ")   \n",
    "display(selectState)\n",
    "select = widgets.Dropdown(\n",
    "    options=_category,\n",
    "    description='Category:',\n",
    "    disabled=False\n",
    ")\n",
    "display(select)\n",
    "    \n",
    "\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        clear_output()\n",
    "        _selectedCat[0] = (change['new'])    \n",
    " \n",
    "def on_change_state(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        clear_output()\n",
    "        _selectedSta[0] = (change['new'])   \n",
    "\n",
    "select.observe(on_change)\n",
    "selectState.observe(on_change_state)\n",
    "clear_output()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Payment and Value care"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ploting for: All, NA\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`M.Measure_ID`' given input columns: [D.Measure start date, D.State, D.Lower estimate, D.Hospital_Name, M.Measure_Name, D.Measure end date, D.Value_of_care_category, D.Higher estimate, M.National payment, D.Denominator, D.Provider ID, D.Phone number, D.Value of care display ID, D.Value of care display name, D.Payment measure name, D.Payment footnote, D.ZIP Code, M.Measure end date, D.Address, M.Number greater than national payment, M.Measure start date, M.Footnote, M.Number less than national payment, M.Payment_measure_ID, D.County name, D.Value of care footnote, D.Payment, D.Payment category, M.Number same as national payment, D.City, M.Number of hospitals too few, D.Payment_measure_ID]; line 1 pos 351;\\n'Sort ['D.Hospital_Name ASC NULLS FIRST], true\\n+- 'Project ['Hospital_Name, 'Payment, 'Value_of_care_category, 'M.National_payment AS th#186, CASE WHEN 'Value_of_care_category LIKE Average mortality% THEN 14817 WHEN 'Value_of_care_category LIKE Better mortality% THEN 15817 WHEN 'Value_of_care_category LIKE Worse mortality% THEN 13817 ELSE null END AS Mor#187]\\n   +- 'Filter (NOT ('Payment = Not Available) && ('Payment_measure_name = NA))\\n      +- 'Join Inner, (Payment_measure_ID#117 = 'M.Measure_ID)\\n         :- SubqueryAlias D\\n         :  +- SubqueryAlias data\\n         :     +- Relation[Provider ID#108,Hospital_Name#109,Address#110,City#111,State#112,ZIP Code#113,County name#114,Phone number#115L,Payment measure name#116,Payment_measure_ID#117,Payment category#118,Denominator#119,Payment#120,Lower estimate#121,Higher estimate#122,Payment footnote#123,Value of care display name#124,Value of care display ID#125,Value_of_care_category#126,Value of care footnote#127,Measure start date#128,Measure end date#129] csv\\n         +- SubqueryAlias M\\n            +- SubqueryAlias master\\n               +- Relation[Measure_Name#162,Payment_measure_ID#163,National payment#164,Number less than national payment#165,Number same as national payment#166,Number greater than national payment#167,Number of hospitals too few#168,Footnote#169,Measure start date#170,Measure end date#171] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`M.Measure_ID`' given input columns: [D.Measure start date, D.State, D.Lower estimate, D.Hospital_Name, M.Measure_Name, D.Measure end date, D.Value_of_care_category, D.Higher estimate, M.National payment, D.Denominator, D.Provider ID, D.Phone number, D.Value of care display ID, D.Value of care display name, D.Payment measure name, D.Payment footnote, D.ZIP Code, M.Measure end date, D.Address, M.Number greater than national payment, M.Measure start date, M.Footnote, M.Number less than national payment, M.Payment_measure_ID, D.County name, D.Value of care footnote, D.Payment, D.Payment category, M.Number same as national payment, D.City, M.Number of hospitals too few, D.Payment_measure_ID]; line 1 pos 351;\n'Sort ['D.Hospital_Name ASC NULLS FIRST], true\n+- 'Project ['Hospital_Name, 'Payment, 'Value_of_care_category, 'M.National_payment AS th#186, CASE WHEN 'Value_of_care_category LIKE Average mortality% THEN 14817 WHEN 'Value_of_care_category LIKE Better mortality% THEN 15817 WHEN 'Value_of_care_category LIKE Worse mortality% THEN 13817 ELSE null END AS Mor#187]\n   +- 'Filter (NOT ('Payment = Not Available) && ('Payment_measure_name = NA))\n      +- 'Join Inner, (Payment_measure_ID#117 = 'M.Measure_ID)\n         :- SubqueryAlias D\n         :  +- SubqueryAlias data\n         :     +- Relation[Provider ID#108,Hospital_Name#109,Address#110,City#111,State#112,ZIP Code#113,County name#114,Phone number#115L,Payment measure name#116,Payment_measure_ID#117,Payment category#118,Denominator#119,Payment#120,Lower estimate#121,Higher estimate#122,Payment footnote#123,Value of care display name#124,Value of care display ID#125,Value_of_care_category#126,Value of care footnote#127,Measure start date#128,Measure end date#129] csv\n         +- SubqueryAlias M\n            +- SubqueryAlias master\n               +- Relation[Measure_Name#162,Payment_measure_ID#163,National payment#164,Number less than national payment#165,Number same as national payment#166,Number greater than national payment#167,Number of hospitals too few#168,Footnote#169,Measure start date#170,Measure end date#171] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:117)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:104)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-01c71fefdc03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# print(query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdistint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#  distint.show();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \"\"\"\n\u001b[0;32m--> 708\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.3.0/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`M.Measure_ID`' given input columns: [D.Measure start date, D.State, D.Lower estimate, D.Hospital_Name, M.Measure_Name, D.Measure end date, D.Value_of_care_category, D.Higher estimate, M.National payment, D.Denominator, D.Provider ID, D.Phone number, D.Value of care display ID, D.Value of care display name, D.Payment measure name, D.Payment footnote, D.ZIP Code, M.Measure end date, D.Address, M.Number greater than national payment, M.Measure start date, M.Footnote, M.Number less than national payment, M.Payment_measure_ID, D.County name, D.Value of care footnote, D.Payment, D.Payment category, M.Number same as national payment, D.City, M.Number of hospitals too few, D.Payment_measure_ID]; line 1 pos 351;\\n'Sort ['D.Hospital_Name ASC NULLS FIRST], true\\n+- 'Project ['Hospital_Name, 'Payment, 'Value_of_care_category, 'M.National_payment AS th#186, CASE WHEN 'Value_of_care_category LIKE Average mortality% THEN 14817 WHEN 'Value_of_care_category LIKE Better mortality% THEN 15817 WHEN 'Value_of_care_category LIKE Worse mortality% THEN 13817 ELSE null END AS Mor#187]\\n   +- 'Filter (NOT ('Payment = Not Available) && ('Payment_measure_name = NA))\\n      +- 'Join Inner, (Payment_measure_ID#117 = 'M.Measure_ID)\\n         :- SubqueryAlias D\\n         :  +- SubqueryAlias data\\n         :     +- Relation[Provider ID#108,Hospital_Name#109,Address#110,City#111,State#112,ZIP Code#113,County name#114,Phone number#115L,Payment measure name#116,Payment_measure_ID#117,Payment category#118,Denominator#119,Payment#120,Lower estimate#121,Higher estimate#122,Payment footnote#123,Value of care display name#124,Value of care display ID#125,Value_of_care_category#126,Value of care footnote#127,Measure start date#128,Measure end date#129] csv\\n         +- SubqueryAlias M\\n            +- SubqueryAlias master\\n               +- Relation[Measure_Name#162,Payment_measure_ID#163,National payment#164,Number less than national payment#165,Number same as national payment#166,Number greater than national payment#167,Number of hospitals too few#168,Footnote#169,Measure start date#170,Measure end date#171] csv\\n\""
     ]
    }
   ],
   "source": [
    "print(\"Ploting for: \"+ _selectedSta[0] +\", \" + _selectedCat[0])\n",
    "\n",
    "if _selectedCat[0] == \"Payment for heart failure patients\":\n",
    "   stateRate = 15959\n",
    "elif _selectedCat[0] == \"Payment for heart attack patients\":\n",
    "   stateRate = 22760 \n",
    "else:\n",
    "   stateRate = 14817\n",
    "\n",
    "# print(t)\n",
    "# print(_selectedSta[0])\n",
    "\n",
    "query = \" SELECT Hospital_Name,Payment,Value_of_care_category,M.National_payment as th\"\n",
    "query += \" ,(CASE WHEN Value_of_care_category LIKE 'Average mortality%' then \" +str(stateRate) \n",
    "query += \" WHEN Value_of_care_category LIKE 'Better mortality%' then \" +str(stateRate + 1000) \n",
    "query += \" WHEN Value_of_care_category LIKE 'Worse mortality%' then \" +str(stateRate - 1000) +\" ELSE NULL END) AS Mor\" \n",
    "query += \" FROM Data D JOIN Master M\"\n",
    "query += \" ON D.Payment_measure_ID = M.Measure_ID\"\n",
    "query += \" WHERE Payment != 'Not Available'\"\n",
    "\n",
    "\n",
    "if _selectedSta[0] != \"All\":\n",
    "    query += \" AND State = '\" +  _selectedSta[0] + \"'\" \n",
    "    \n",
    "\n",
    "query += \" AND Payment_measure_name = '\" +  _selectedCat[0] + \"'\"\n",
    "query += \" ORDER by D.Hospital_Name\"\n",
    "\n",
    "# print(query)\n",
    "distint = spark.sql(query)\n",
    "#  distint.show();\n",
    "\n",
    "\n",
    "\n",
    "_x=[]\n",
    "_y=[]\n",
    "_text=[]\n",
    "_th = []\n",
    "_Mor =[]\n",
    "_dataList = distint.collect()\n",
    "for i in range(len(_dataList)):\n",
    "    _x.append(_dataList[i][0])\n",
    "    _y.append(_dataList[i][1])\n",
    "    _th.append(_dataList[i][3])\n",
    "    _text.append(_dataList[i][2])\n",
    "    _Mor.append(_dataList[i][4])\n",
    "    \n",
    "# print(_selectedCat[0])\n",
    "\n",
    "import plotly\n",
    "# Set Plotly credetials\n",
    "\n",
    "plotly.tools.set_credentials_file(username='keefeseanc', api_key='A8CaFbFKVZZjBQLnT4QI')\n",
    "import plotly.plotly as py\n",
    "import plotly.tools as plotly_tools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.graph_objs import *\n",
    " #Sign in the plotly API\n",
    "py.sign_in(\"keefeseanc\", \"A8CaFbFKVZZjBQLnT4QI\")\n",
    "\n",
    "Payment = go.Scatter(\n",
    "    x=_x,\n",
    "    y=_y,\n",
    "    name = 'Payment',\n",
    "    text = _text\n",
    ")\n",
    "\n",
    "Mort = go.Scatter(\n",
    "    x=_x,\n",
    "    y=_Mor,\n",
    "    name = 'Mortality',\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "National_Rate = go.Scatter(\n",
    "    x=_x,\n",
    "    y=_th,\n",
    "    name = 'National Rate'\n",
    ")\n",
    "\n",
    "data = [Payment,National_Rate,Mort]\n",
    "py.iplot(data, filename='basic-line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
